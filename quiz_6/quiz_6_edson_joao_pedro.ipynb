{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quiz_6_edson_joao-pedro.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvyMZaXxqKNF",
        "colab_type": "text"
      },
      "source": [
        "# Quiz 6 - Object Recognition: BoF vs ConvNets\n",
        "#### Edson Roteia Araujo Junior e João Pedro Moreira Ferreira"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmdijAdVqKNH",
        "colab_type": "text"
      },
      "source": [
        "### Instructions\n",
        "The goal of this quiz is to implement two object recognition approaches:\n",
        "\n",
        "1.     A classifier based on bag of features:\n",
        "\n",
        "    - Use SVM or Random Forest as classifiers.\n",
        "\n",
        "    - Try different sizes for the dictionary.\n",
        "\n",
        "2.     A classifier using ConvNet implemented in Keras:\n",
        " \n",
        "     - Use an architecture inspired in the LeNet5\n",
        "\n",
        "Your code must be implemented on a notebook python and you must use the CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html) for training and testing.\n",
        "\n",
        "The notebook must present a confusion matrix and the average accuracy for each approach. You also have to report both training and test accuracies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0UCLTOyqKNI",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Features ###\n",
        "\n",
        "#### In order to enable the SITF module of the OpenCV library.<span style=\"color:red\"> We install the opencv-contrib-python at version 3.4.2.16.</span> You should install it to make this notebook work. You can install it through any python package manager, since the module is in a pypi repository. More informations [here](https://pypi.org/project/opencv-contrib-python/3.4.2.16/) ####\n",
        "\n",
        "#### Also you may need to install the <span style=\"color:red\">scikit-image</span> module. Again it can be installed by your python package manager ####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ8JAgDkrw_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "e6f4a581-c3c2-4657-e657-f038a7d97439"
      },
      "source": [
        "!pip install opencv-contrib-python==3.4.2.16"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-contrib-python==3.4.2.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/1d/e5e7c01fba5ae64abbf76cb3d38ffb3958c38b46ec6292166e549dde75a1/opencv_contrib_python-3.4.2.16-cp27-cp27mu-manylinux1_x86_64.whl (30.6MB)\n",
            "\u001b[K     |████████████████████████████████| 30.6MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from opencv-contrib-python==3.4.2.16) (1.16.4)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "  Found existing installation: opencv-contrib-python 3.4.3.18\n",
            "    Uninstalling opencv-contrib-python-3.4.3.18:\n",
            "      Successfully uninstalled opencv-contrib-python-3.4.3.18\n",
            "Successfully installed opencv-contrib-python-3.4.2.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWUCuJwoqKNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "e554a729-b346-4cbb-d2ab-e29905bd0f36"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "##CARREGAR DATASET###\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrg1IalsqKNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage import img_as_ubyte\n",
        "\n",
        "def featureExtraction(sift,image):\n",
        "    image = img_as_ubyte(image)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "    return keypoints, descriptors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-1lK7BhqKNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "d5094832-d66b-4bbb-c847-8999b3e5f85c"
      },
      "source": [
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "descriptors_list = []\n",
        "for image in X_train:\n",
        "    keypoints, descriptors = featureExtraction(sift,image)\n",
        "    descriptors_list.append(descriptors)\n",
        "    \n",
        "print(len(descriptors_list))\n",
        "print(descriptors_list[0].shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float32 to uint8\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "(14, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aW0RJG1w4Kh",
        "colab_type": "text"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjCZASKbqKNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "09f437aa-4069-4246-9efd-5659f4a1d1f4"
      },
      "source": [
        "def build_histogram(descriptor_list, cluster_alg):\n",
        "    histogram = np.zeros(len(cluster_alg.cluster_centers_))\n",
        "    cluster_result =  cluster_alg.predict(descriptor_list)\n",
        "    for i in cluster_result:\n",
        "        histogram[i] += 1.0\n",
        "    return histogram\n",
        "\n",
        "print(descriptors_list[0].shape)\n",
        "flatten_descriptors_list = []\n",
        "for sublist in descriptors_list:\n",
        "    if not sublist is None:  \n",
        "      for item in sublist:\n",
        "          flatten_descriptors_list.append(item)\n",
        "print(\"Finished creating flatten descriptor vectors!\")\n",
        "\n",
        "k_values = [10]\n",
        "for k in k_values:\n",
        "  kmeans = KMeans(n_clusters = k)\n",
        "  kmeans.fit(flatten_descriptors_list)\n",
        "  image_bags = []\n",
        "  for image_descriptors in descriptors_list:\n",
        "    if(image_descriptors is not None):\n",
        "      histogram = build_histogram(image_descriptors, kmeans)\n",
        "      image_bags.append(histogram)\n",
        "  print(len(image_bags))\n",
        "  print(image_bags[0].shape)\n",
        "  print(image_bags[1].shape)\n",
        "  neighbor = NearestNeighbors(n_neighbors = 20)\n",
        "  neighbor.fit(image_bags)\n",
        "  dist, result = neighbor.kneighbors([image_bags[0]])\n",
        "  print(dist)\n",
        "  print(result)\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 128)\n",
            "Finished creating flatten descriptor vectors!\n",
            "49906\n",
            "(10,)\n",
            "(10,)\n",
            "[[0.         1.         1.41421356 1.41421356 1.41421356 1.73205081\n",
            "  1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
            "  1.73205081 1.73205081 1.73205081 1.73205081 2.         2.\n",
            "  2.         2.        ]]\n",
            "[[    0 42667 21795 47612 23830 49188     3  8636  8694 21878 37978 35984\n",
            "  38798 30039 41359 41639  4360 15749  8376 49318]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcAvNnkT3b7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWBWZdz0qKNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(descriptors_list[0].shape,descriptors_list[1].shape,descriptors_list[2].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXcaVhXiqKNy",
        "colab_type": "text"
      },
      "source": [
        "### ConvNet ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb3C0mMKqKN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot ad hoc CIFAR10 instances\n",
        "from keras.datasets import cifar10\n",
        "from matplotlib import pyplot\n",
        "from PIL import Image\n",
        "# from scipy.misc import toimage -> DEPRECATED\n",
        "from keras.layers import *\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTw5M7dhqKO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a grid of 3x3 images\n",
        "for i in range(0, 9):\n",
        "\tpyplot.subplot(330 + 1 + i)\n",
        "\tpyplot.imshow(Image.fromarray(X_train[i]))\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "910LhcIBqKPQ",
        "colab_type": "text"
      },
      "source": [
        "#### Simple Covolutional Neural Network for CIFAR-10 ####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-YCz6ymqKPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from keras.layers import *\n",
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Compile model\n",
        "epochs = 25\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a0M7nWnqKQR",
        "colab_type": "text"
      },
      "source": [
        "#### Larger Covolutional Neural Network for CIFAR-10 ####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0dvoshIqKQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Compile model\n",
        "epochs = 25\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "numpy.random.seed(seed)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3E2Qr2kqKQf",
        "colab_type": "text"
      },
      "source": [
        "#### Architecture Inspired in the LeNet5 ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odcKJDqpqKQg",
        "colab_type": "text"
      },
      "source": [
        "![lenet5](imgs/lenet5.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNfA6DeHqKQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(3,32,32)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=120, activation='relu'))\n",
        "model.add(Dense(units=84, activation='relu'))\n",
        "model.add(Dense(units=10, activation = 'softmax'))\n",
        "\n",
        "epochs = 25\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "numpy.random.seed(seed)\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQxSeMBaqKQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}